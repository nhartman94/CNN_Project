{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "Experimenting with the API for creating DataLoaders for the train, val and test sets!\n",
    "\n",
    "**Plan:**\n",
    "\n",
    "- Create datasets for the $e$ / $\\gamma$ / $\\pi$ files\n",
    "- Concatenate the datasets\n",
    "- Then Dataloader to make the train, val, and test sets. \n",
    "**Important:** You just have to be careful when passing the list indices so that you equally sample from all 3 of the classes. \n",
    "- Use a 60:10:30 split for the train:val:test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create a Dataset class\n",
    "\n",
    "In the Pytorch HW set from class, they used the pre-defined CIFAR10 dataset which was already pre-loaded, but I'm going to need to write my own Dataset class that subclasses the `Dataset` class.\n",
    "\n",
    "I'm following along with Pytorch's [data loading tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emShowersDatasetFlat(Dataset):\n",
    "    \"\"\"EM showers dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, relPath='../data', N=100000):\n",
    "        \"\"\"\n",
    "        \n",
    "        Instantiates a class which then returns examples as a tuple for the \n",
    "        image labels, and the truth labels are:\n",
    "            0 (gamma), 1 (pi-plus), 2 (positron)\n",
    "        \n",
    "        Args:\n",
    "            relPath: The relative path to where the hdf5 files live\n",
    "            N: The number of images we have for each particle class\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        d_gamma  = h5py.File('../data/gamma.hdf5', 'r')\n",
    "        d_piplus = h5py.File('../data/piplus.hdf5', 'r')\n",
    "        d_eplus  = h5py.File('../data/eplus.hdf5', 'r')\n",
    "        \n",
    "        layer0 = np.vstack((d_gamma['layer_0'][:], d_piplus['layer_0'][:], d_eplus['layer_0'][:]))\n",
    "        layer1 = np.vstack((d_gamma['layer_1'][:], d_piplus['layer_1'][:], d_eplus['layer_1'][:]))\n",
    "        layer2 = np.vstack((d_gamma['layer_2'][:], d_piplus['layer_2'][:], d_eplus['layer_2'][:]))\n",
    "        \n",
    "        # Test to make sure that all of the datasets are the same length\n",
    "        self.layer0 = torch.from_numpy(layer0)\n",
    "        self.layer1 = torch.from_numpy(layer1)\n",
    "        self.layer2 = torch.from_numpy(layer2)\n",
    "        \n",
    "        # Get the y labels\n",
    "        self.y = torch.from_numpy(np.concatenate((np.zeros(N), np.ones(N), 2*np.ones(N))))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.layer0.shape[0] \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.layer0[idx], self.layer1[idx], self.layer2[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create a DataLoader\n",
    "\n",
    "So the only thing here is we have to be careful with the train / val / test split to get equal proportions in all of the classes and make sure the SubsetRandomSampler is drawing equal proportions for all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = emShowersDatasetFlat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000 # 100k events / particle\n",
    "nClasses = 3\n",
    "\n",
    "trainFrac = .7\n",
    "valFrac = .1\n",
    "testFrac = .3\n",
    "\n",
    "idxTrain = []\n",
    "idxVal = []\n",
    "idxTest = []\n",
    "\n",
    "for i in range(nClasses):\n",
    "    \n",
    "    idxTrain += [j for j in range(i*N, int((i+trainFrac)*N))]\n",
    "    idxVal += [j for j in range(int((i+trainFrac)*N) + int((i+trainFrac+valFrac)*N))]\n",
    "    idxTest += [j for j in range(int((i+trainFrac+valFrac)*N), (i+1)*N)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "loader_train = DataLoader(dset, batch_size=batch_size, sampler=SubsetRandomSampler(idxTrain))\n",
    "loader_val = DataLoader(dset, batch_size=batch_size, sampler=SubsetRandomSampler(idxVal))\n",
    "loader_test = DataLoader(dset, batch_size=batch_size, sampler=SubsetRandomSampler(idxTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you can successfully iterate over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 96])\n",
      "torch.Size([64, 12, 12])\n",
      "torch.Size([64, 12, 6])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for layer0, layer1, layer2, y in loader_train:\n",
    "    print(layer0.shape)\n",
    "    print(layer1.shape)\n",
    "    print(layer2.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Subtract the mean image and divide by the standard deviation for each layer\n",
    "\n",
    "This was the only prepreocessing step that we did for the CIFAR 10 dataset, so I thought maybe it would be useful to put here as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
