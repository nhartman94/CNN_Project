{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3d CNN\n",
    "\n",
    "**Goals:** \n",
    "\n",
    "1. Play with the dimension uniformity of the three layers\n",
    "2. Use the uniform dimension layers to train our first 3d CNN model!\n",
    "3. Start doing some coarse playing with hyperparameters interactively to understand the problem better :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Loading the other modules I have in parent directories\n",
    "import sys\n",
    "PYTHONPATH=\"../\"\n",
    "sys.path.append(PYTHONPATH)\n",
    "from dataProcessing import getDataLoaders\n",
    "from models import FCNet, CNN_3d, layer0_12x12, layer2_12x12, layer0_3x6, count_parameters\n",
    "from train import check_accuracy, train\n",
    "from plottingFcts import trainingMetrics, sigBkgEff, plotROC, plotConfusion\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train, loader_val, loader_test = getDataLoaders(batch_size=256, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-6.3521e-02, -3.5718e-02, -2.9265e-02,  ..., -2.0367e-02,\n",
      "           -4.8355e-02, -9.8583e-03],\n",
      "          [-6.2822e-02, -6.1457e-02, -1.1775e-01,  ...,  6.4844e-01,\n",
      "           -2.7774e-02, -5.2298e-02],\n",
      "          [-3.8525e-02, -7.0110e-02, -4.8287e-02,  ..., -1.7421e-02,\n",
      "           -1.9006e-02, -2.4024e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2724e+00, -3.5718e-02, -2.9265e-02,  ..., -2.0367e-02,\n",
      "           -4.8355e-02, -9.8583e-03],\n",
      "          [-6.2822e-02, -6.1457e-02, -1.1775e-01,  ..., -3.4048e-02,\n",
      "           -4.0244e-02, -5.2298e-02],\n",
      "          [-4.1269e-02, -7.0110e-02, -4.8287e-02,  ..., -1.7421e-02,\n",
      "           -1.9006e-02, -2.4024e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.3521e-02, -3.5718e-02, -2.9265e-02,  ..., -2.0367e-02,\n",
      "           -4.8355e-02, -9.8583e-03],\n",
      "          [ 3.1008e-01, -6.1457e-02, -1.1775e-01,  ..., -3.4048e-02,\n",
      "           -4.0244e-02, -5.2298e-02],\n",
      "          [-4.1269e-02, -7.0110e-02, -4.8287e-02,  ..., -1.7421e-02,\n",
      "           -1.9006e-02, -2.4024e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-6.3521e-02, -3.5718e-02, -2.9265e-02,  ..., -2.0367e-02,\n",
      "           -4.8355e-02, -9.8583e-03],\n",
      "          [-6.2822e-02, -6.1457e-02, -1.1775e-01,  ..., -3.4048e-02,\n",
      "           -4.0244e-02, -5.2298e-02],\n",
      "          [-4.1269e-02, -7.0110e-02, -4.8287e-02,  ..., -1.7421e-02,\n",
      "           -1.9006e-02, -2.4024e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.3521e-02, -3.5718e-02, -2.9265e-02,  ..., -2.0367e-02,\n",
      "           -4.8355e-02, -9.8583e-03],\n",
      "          [-6.2822e-02, -6.1457e-02,  3.2307e+00,  ..., -3.4048e-02,\n",
      "           -4.0244e-02, -5.2298e-02],\n",
      "          [-4.1269e-02, -7.0110e-02, -4.8287e-02,  ..., -1.7421e-02,\n",
      "           -1.9006e-02, -2.4024e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.3521e-02, -3.5718e-02, -2.9265e-02,  ..., -2.0367e-02,\n",
      "           -4.8355e-02, -9.8583e-03],\n",
      "          [-5.2806e-02,  4.2371e-01,  1.1114e+00,  ..., -3.4048e-02,\n",
      "           -4.0244e-02, -5.2298e-02],\n",
      "          [-4.1269e-02, -7.0110e-02, -4.8287e-02,  ..., -1.7421e-02,\n",
      "           -1.9006e-02, -2.4024e-02]]]]), tensor([[[[-3.4694e-01,  1.5374e+00,  2.2236e+00,  ...,  4.1166e+00,\n",
      "            3.1526e+00, -3.4676e-01],\n",
      "          [-4.5516e-01,  1.5399e+01, -9.6121e-01,  ..., -1.6750e+00,\n",
      "            1.6564e+00,  2.1987e+00],\n",
      "          [ 6.4741e+00, -8.1742e-01,  1.7081e+01,  ..., -7.5868e-01,\n",
      "            1.9800e+00, -8.6166e-02],\n",
      "          ...,\n",
      "          [-8.8865e-01,  5.9049e-01,  8.2921e+00,  ...,  1.8043e+00,\n",
      "           -8.8358e-01,  3.8745e-01],\n",
      "          [ 4.3363e-01,  9.1658e+00,  1.7166e+00,  ...,  4.8535e+00,\n",
      "           -1.2308e+00, -3.0156e-01],\n",
      "          [-5.3423e-01, -5.1779e-01,  1.9454e+00,  ..., -7.6982e-01,\n",
      "           -1.1327e+00, -4.9818e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.4694e-01, -4.2584e-01,  1.4769e-02,  ..., -1.1985e+00,\n",
      "           -4.2904e-01, -9.6453e-02],\n",
      "          [-4.8753e-01,  4.0005e-01,  1.9367e+00,  ...,  1.2644e+00,\n",
      "            2.7468e+00, -3.4911e-01],\n",
      "          [-1.1895e+00, -1.6382e+00,  2.3216e+01,  ...,  1.0450e+00,\n",
      "           -2.6162e+00, -4.0838e-02],\n",
      "          ...,\n",
      "          [-8.7998e-01, -2.4826e-01,  8.1055e+00,  ..., -4.7611e+00,\n",
      "            3.5934e-01,  9.7147e-01],\n",
      "          [-6.2946e-01,  9.4753e-01, -1.4914e+00,  ..., -1.8460e+00,\n",
      "           -1.3319e+00, -3.0244e-01],\n",
      "          [-5.3562e-01, -5.1902e-01, -2.7513e-01,  ..., -7.6939e-01,\n",
      "           -1.5080e+00, -4.9818e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.4694e-01, -4.4463e-01,  1.6821e-02,  ..., -1.1985e+00,\n",
      "           -4.2904e-01, -3.4676e-01],\n",
      "          [ 1.2471e-01,  6.9091e-01, -1.6769e+00,  ..., -5.9879e-01,\n",
      "           -8.0513e-01, -3.4911e-01],\n",
      "          [-1.1895e+00, -2.3065e+00, -5.0734e-01,  ..., -1.6855e+00,\n",
      "           -2.9883e+00, -7.9741e-01],\n",
      "          ...,\n",
      "          [ 2.7136e+00, -6.3796e-01,  1.1702e+00,  ..., -4.3445e+00,\n",
      "           -1.7620e+00,  1.3975e+00],\n",
      "          [-6.3703e-01, -1.1119e-01,  1.4935e+00,  ...,  1.1709e-01,\n",
      "           -1.3319e+00, -3.0244e-01],\n",
      "          [-5.3562e-01, -5.1902e-01,  4.3695e+00,  ..., -7.6982e-01,\n",
      "           -1.5080e+00, -4.9818e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.4694e-01, -2.5130e-01, -1.1621e+00,  ..., -1.1985e+00,\n",
      "           -4.2904e-01, -3.4676e-01],\n",
      "          [ 8.7085e-01,  1.7768e+00,  1.0271e+00,  ...,  6.6945e-01,\n",
      "           -5.0689e-01,  2.7036e+00],\n",
      "          [-1.1895e+00,  3.9097e+00,  1.5883e+00,  ..., -1.6787e+00,\n",
      "           -1.7547e+00, -9.5344e-02],\n",
      "          ...,\n",
      "          [ 5.0672e-01, -1.3254e+00, -1.6158e+00,  ..., -4.2509e+00,\n",
      "           -1.9987e+00,  2.1142e+00],\n",
      "          [-6.3703e-01, -8.9578e-01,  4.8273e+00,  ...,  4.3810e+00,\n",
      "           -1.3319e+00,  1.9251e+00],\n",
      "          [-5.3562e-01, -5.1902e-01,  2.5075e+00,  ..., -7.6278e-01,\n",
      "           -1.5080e+00, -4.9818e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.1333e-01,  6.2476e-01,  4.5316e-01,  ...,  8.1789e+00,\n",
      "           -4.2904e-01, -3.4676e-01],\n",
      "          [ 1.5092e+00, -1.2654e+00,  2.9762e-01,  ...,  1.1177e+00,\n",
      "           -6.3399e-01, -3.4911e-01],\n",
      "          [ 1.4127e+00,  7.9684e-01, -2.2150e+00,  ...,  1.6044e+00,\n",
      "            4.4426e+00, -7.9741e-01],\n",
      "          ...,\n",
      "          [ 6.3304e+00,  1.3215e+00,  2.2501e+01,  ...,  1.0596e+00,\n",
      "            6.9331e-02,  2.4403e+00],\n",
      "          [-1.9364e-01,  1.2328e+00,  8.3913e-01,  ..., -1.9387e+00,\n",
      "           -1.3319e+00,  5.7149e-01],\n",
      "          [-5.3562e-01,  3.7383e-01,  1.0377e+00,  ..., -7.6349e-01,\n",
      "           -1.0593e+00, -4.9818e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.4694e-01,  6.2922e-01, -2.0663e-01,  ..., -1.1985e+00,\n",
      "           -4.2638e-01,  3.0783e-01],\n",
      "          [-4.8753e-01, -5.0014e-01,  1.9818e+00,  ..., -8.7486e-01,\n",
      "            2.6205e-01, -3.4911e-01],\n",
      "          [-1.5783e-01,  2.2706e+00,  1.6502e+01,  ...,  2.2792e-01,\n",
      "           -2.9883e+00,  5.1296e-01],\n",
      "          ...,\n",
      "          [-8.9097e-01,  3.8763e+00, -2.0106e+00,  ...,  2.8130e+00,\n",
      "            5.8168e+00,  1.1075e+01],\n",
      "          [ 6.4347e-01, -8.9578e-01, -2.1006e+00,  ...,  2.0958e+00,\n",
      "            1.7571e+00,  4.8315e-01],\n",
      "          [ 2.4764e+00, -8.0245e-03,  6.7566e-01,  ..., -7.6982e-01,\n",
      "            2.9697e-01, -4.9818e-01]]]]), tensor([[[[-4.5068e-02,  9.5099e-01,  2.8420e+00, -7.6283e-02, -1.2735e-01,\n",
      "           -5.7143e-02],\n",
      "          [-9.0104e-02, -3.0136e-01,  4.1425e-01, -4.9127e-01, -4.9406e-01,\n",
      "           -1.2300e-01],\n",
      "          [ 4.4425e+00,  8.0260e-02,  2.3064e+00,  6.3643e-01, -9.6973e-01,\n",
      "           -1.1015e-01],\n",
      "          ...,\n",
      "          [-1.7605e-01, -7.4899e-01, -1.1495e+00, -1.8182e-01,  5.9161e-02,\n",
      "           -1.1671e-01],\n",
      "          [-8.6791e-02, -2.0993e-01, -5.1185e-01,  9.9718e-01, -3.2329e-01,\n",
      "           -1.1795e-01],\n",
      "          [-1.5000e-02, -7.2274e-02, -2.6530e-01, -1.2863e-01, -3.5976e-01,\n",
      "           -5.9183e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.5068e-02, -2.0227e-01, -3.2048e-01, -1.8421e-03, -1.2735e-01,\n",
      "           -5.7143e-02],\n",
      "          [-9.0104e-02, -4.5006e-01, -5.7665e-01, -1.8838e-01, -1.0034e-01,\n",
      "           -1.2300e-01],\n",
      "          [ 1.8406e+00,  2.2586e+00, -6.3043e-01, -1.8010e+00,  2.3939e-01,\n",
      "           -1.1015e-01],\n",
      "          ...,\n",
      "          [-1.7605e-01, -7.4899e-01,  4.3672e-01,  4.2593e+00,  4.7368e-01,\n",
      "           -1.1671e-01],\n",
      "          [-8.6791e-02, -2.0993e-01,  5.0040e-01, -1.1322e+00, -3.2329e-01,\n",
      "           -1.1795e-01],\n",
      "          [-3.2395e-02, -7.2274e-02, -2.6530e-01, -1.2863e-01, -3.5976e-01,\n",
      "           -5.9183e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.5068e-02, -2.0227e-01, -3.2048e-01, -7.6283e-02, -1.2735e-01,\n",
      "           -5.7143e-02],\n",
      "          [-9.0104e-02, -4.5006e-01,  5.0497e-02, -2.3967e-01, -4.9406e-01,\n",
      "           -1.2300e-01],\n",
      "          [-5.8212e-01, -3.7494e-01, -1.1414e+00, -1.8010e+00,  1.0187e+00,\n",
      "           -1.1015e-01],\n",
      "          ...,\n",
      "          [-1.7605e-01, -7.4899e-01,  1.4045e+00, -1.4275e+00, -4.5184e-01,\n",
      "           -1.1671e-01],\n",
      "          [-8.6791e-02, -2.0993e-01, -8.5591e-04,  8.4055e-02, -3.2329e-01,\n",
      "           -1.1795e-01],\n",
      "          [-3.2395e-02, -7.2274e-02, -2.6530e-01, -1.2863e-01, -3.5976e-01,\n",
      "           -5.9183e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.5068e-02, -2.0227e-01, -3.2048e-01, -7.6283e-02, -1.2735e-01,\n",
      "           -5.7143e-02],\n",
      "          [-9.0104e-02, -4.5006e-01, -2.2080e-01, -4.9127e-01,  7.5185e-01,\n",
      "           -1.2300e-01],\n",
      "          [-5.8212e-01, -3.7494e-01, -1.1414e+00, -1.8010e+00, -9.6973e-01,\n",
      "           -1.1015e-01],\n",
      "          ...,\n",
      "          [-1.7605e-01, -7.4899e-01, -1.1495e+00, -6.8638e-01, -4.5184e-01,\n",
      "           -1.1671e-01],\n",
      "          [-8.6791e-02, -2.0993e-01, -5.1185e-01, -1.1322e+00, -3.2329e-01,\n",
      "           -1.1795e-01],\n",
      "          [-3.2395e-02, -7.2274e-02, -2.6530e-01, -1.2863e-01, -3.5976e-01,\n",
      "           -5.9183e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.5068e-02, -2.0227e-01,  4.7538e-01, -7.6283e-02, -1.2735e-01,\n",
      "           -5.7143e-02],\n",
      "          [-9.0104e-02,  9.2423e-01, -5.7665e-01, -4.9127e-01,  7.0396e-01,\n",
      "           -1.2300e-01],\n",
      "          [-5.8212e-01,  3.0224e+00,  4.5968e-01, -1.4219e+00, -1.7142e-01,\n",
      "           -1.1015e-01],\n",
      "          ...,\n",
      "          [-1.7605e-01, -7.4899e-01, -1.1495e+00, -2.2873e+00, -4.5184e-01,\n",
      "           -1.1671e-01],\n",
      "          [-8.6791e-02, -2.0993e-01, -8.5591e-04, -1.1322e+00, -3.2329e-01,\n",
      "           -1.1795e-01],\n",
      "          [-3.2395e-02, -7.2274e-02, -2.6530e-01,  6.1823e-01, -3.5976e-01,\n",
      "           -5.9183e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1886e+00, -2.0227e-01, -3.2048e-01, -7.6283e-02, -1.2735e-01,\n",
      "           -5.7143e-02],\n",
      "          [-9.0104e-02, -4.5006e-01, -5.7665e-01,  9.1594e-01, -4.9406e-01,\n",
      "           -1.2300e-01],\n",
      "          [-5.8212e-01, -3.7494e-01, -1.1414e+00,  2.3012e-01, -9.6973e-01,\n",
      "           -1.1015e-01],\n",
      "          ...,\n",
      "          [-1.7605e-01, -7.4899e-01,  2.4167e+00, -2.2873e+00, -3.9100e-01,\n",
      "           -1.1671e-01],\n",
      "          [-8.6791e-02, -2.0993e-01,  1.0381e+00, -1.1322e+00, -3.2329e-01,\n",
      "           -1.1795e-01],\n",
      "          [-3.2395e-02, -7.2274e-02, -2.6530e-01,  1.1125e-01, -3.5976e-01,\n",
      "           -5.9183e-02]]]]), tensor([ 2.,  2.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  2.,\n",
      "         2.,  0.,  1.,  1.,  0.,  1.,  2.,  1.,  0.,  0.,  0.,  0.,\n",
      "         2.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  2.,\n",
      "         0.,  2.,  1.,  0.,  0.,  2.,  1.,  2.,  2.,  1.,  1.,  0.,\n",
      "         2.,  0.,  2.,  0.,  1.,  0.,  1.,  1.,  2.,  1.,  1.,  0.,\n",
      "         2.,  1.,  2.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  2.,\n",
      "         0.,  2.,  1.,  1.,  2.,  2.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
      "         2.,  2.,  2.,  0.,  1.,  0.,  2.,  1.,  0.,  0.,  2.,  1.,\n",
      "         2.,  0.,  2.,  2.,  2.,  2.,  0.,  0.,  1.,  1.,  1.,  2.,\n",
      "         2.,  0.,  2.,  0.,  0.,  1.,  2.,  1.,  1.,  0.,  1.,  2.,\n",
      "         1.,  0.,  2.,  1.,  0.,  1.,  2.,  1.,  2.,  2.,  0.,  1.,\n",
      "         2.,  2.,  2.,  1.,  2.,  0.,  2.,  1.,  1.,  2.,  0.,  0.,\n",
      "         2.,  1.,  2.,  1.,  1.,  2.,  2.,  2.,  1.,  2.,  0.,  0.,\n",
      "         1.,  1.,  2.,  0.,  1.,  2.,  0.,  1.,  1.,  1.,  2.,  0.,\n",
      "         2.,  2.,  0.,  1.,  1.,  1.,  2.,  2.,  2.,  1.,  0.,  2.], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "for i,x in loader_train:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU()\n",
      "  (7): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU()\n",
      "  (11): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (14): ReLU()\n",
      "  (15): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(layer0_3x6(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size after the first conv: 32,3.0,3.0,6.0\n",
      "Output size after the second conv: 16,2.0,2.0,4.0\n",
      "CNN_3d(\n",
      "  (layer0_preConv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer1_preConv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2_preConv): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (cnn3d_1): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "  (bn3d_1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (cnn3d_2): Conv3d(32, 16, kernel_size=(3, 3, 2), stride=(2, 2, 2), padding=(1, 1, 1))\n",
      "  (bn3d_2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=256.0, out_features=150, bias=True)\n",
      "  (fc2): Linear(in_features=150, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=3, bias=True)\n",
      "  (bn1): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "myModel = CNN_3d(spatialDim=(3,6),preConvParams={'nF':32},\n",
    "                 nFilters_1=32, filter_1= (3,3,3), stride_1=(1,1,1), padding_1=(1,1,1),\n",
    "                 nFilters_2=16, filter_2= (3,3,2), stride_2=(2,2,2), padding_2=(1,1,1),\n",
    "                 h1_dim=150, h2_dim=100)\n",
    "print(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cnn3d_3x6_C32_F333_S111_P111_C16_F332_S222_P111_fc_150_100_dpt_0.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel.modelName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check:**\n",
    "Look at the model accuracy before you start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Got 12461 / 30000 correct (41.54)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(loader_val, myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(myModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, close to 33%, as expected!\n",
    "\n",
    "Next, checkt the size of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t, (l0, l1, l2, y) in enumerate(loader_train):\n",
    "# #     l0_12x12 = layer0_12x12(l0)\n",
    "# #     print(\"l0:\",l0_12x12.shape)\n",
    "    \n",
    "# #     print(l2.shape)\n",
    "# #     l2_12x12 = layer2_12x12(l2)\n",
    "# #     print(\"l2:\",l2_12x12.shape)\n",
    "    \n",
    "#     cnn_out = myModel(l0,l1,l2)\n",
    "#     print(cnn_out.shape)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: I want to make sure that I'm stitching the models together correctly, so check the parameters of the uniforming dimensionalaity layers so that I have a baseline that I can check against again after I'm done with the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Parameters in layer0_12x12:')\n",
    "# for p in list(layer0_12x12.parameters()):\n",
    "#     print(p)\n",
    "    \n",
    "# print('\\nParameters in layer2_12x12:')\n",
    "# for p in list(layer2_12x12.parameters()):\n",
    "#     print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Ok, this is exciting!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(myModel.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/60:\n",
      "Iteration 0, loss = 1.1019\n",
      "cpu\n",
      "Got 10002 / 30000 correct (33.34)\n",
      "\n",
      "Iteration 100, loss = 0.9316\n",
      "cpu\n",
      "Got 18437 / 30000 correct (61.46)\n",
      "\n",
      "Iteration 200, loss = 0.8494\n",
      "cpu\n",
      "Got 18925 / 30000 correct (63.08)\n",
      "\n",
      "Iteration 300, loss = 0.7683\n",
      "cpu\n",
      "Got 19362 / 30000 correct (64.54)\n",
      "\n",
      "Iteration 400, loss = 0.7944\n",
      "cpu\n",
      "Got 19834 / 30000 correct (66.11)\n",
      "\n",
      "Iteration 500, loss = 0.7240\n",
      "cpu\n",
      "Got 20324 / 30000 correct (67.75)\n",
      "\n",
      "Iteration 600, loss = 0.7108\n",
      "cpu\n",
      "Got 20630 / 30000 correct (68.77)\n",
      "\n",
      "Iteration 700, loss = 0.6649\n",
      "cpu\n",
      "Got 21101 / 30000 correct (70.34)\n",
      "\n",
      "cpu\n",
      "Got 126237 / 180000 correct (70.13)\n",
      "cpu\n",
      "Got 21022 / 30000 correct (70.07)\n",
      "\n",
      "Epoch 2/60:\n",
      "Iteration 0, loss = 0.6930\n",
      "cpu\n",
      "Got 21084 / 30000 correct (70.28)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-70cb99b5240a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history, myModel = train(loader_train, loader_val, myModel, \n\u001b[0;32m----> 2\u001b[0;31m                          optimizer, epochs=60, returnBest=True)\n\u001b[0m",
      "\u001b[0;32m~/CNN_Project/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader_train, loader_val, model, optimizer, epochs, returnBest)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# This is the backwards pass: compute the gradient of the loss with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# respect to each  parameter of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# Actually update the parameters of the model using the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history, myModel = train(loader_train, loader_val, myModel, \n",
    "                         optimizer, epochs=60, returnBest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "1. Try increasing the number of nodes in the output layer to see if you're losing too much info from these initial convolutions.\n",
    "The acc got up to about 74.5%, but was then plateauing, so I stopped the the training. Then... I tried adding another hidden layer (so 3 fc layers before classification) to see if I was eroding too much of the model info when I was doing these up and down sampling pre-convolutions. The accuracy got up to 75.47%, so I'm going to try *decreasing* the learning rate by a factor of 10 (so now $\\alpha=10^{-5}$) and trying another 10 epochs of training, and that got it down to 75.86% accuracy.\n",
    "Just to be sure I knew *where* my improvements were coming from, and that I was comparing apples to apples, I tried retraining with two final fc layers using $\\alpha=10^{-4}$ for 10 epochs, and then $\\alpha=10^{-5}$ for another 10 epochs. This didn't help my validation accuracy at all, but I decided not to use 3 fc layers for my next few experiments, keeping in mind that these are the types of gains I should be able to achieve by tweaking the convolutional layers.\n",
    "\n",
    "2. Next I tried doubling the number of filters in my two 3d convolutional layers to 32 and 16. This helped a little bit (75% accuracy), but I think the issue is that I need a better pre-convolutional architecture.\n",
    "\n",
    "3. Try using 8 channels for each of the pre-conv layers\n",
    "    - For layer0, I ended up using 4 filters for the downsampling convolution, and 8 filters for the upsampling convolution.\n",
    "    Now I'm at 75% within 3 epochs... (not bad), and my validation acc after 10 epochs with $\\alpha=1e-4$ is 75.68%. After training another 10 epochs with $\\alpha=1e-5$, the val acc gets up to 75.87%\n",
    "    \n",
    "4. How far can this \"new idea\" take us? Try increasing the number of these preprocessing channels to 16 $\\rightarrow$ 76.46% accuracy \n",
    "\n",
    "5. For layer2, instead of just duplicating this layer 16 times, try a filter with $3\\times3$ filters that is weight preserving so you can learn different higher level features from this repn while preserving the spatial information.\n",
    "I got to ()% accuracy, so I don't really think that that helped. But now that I'm  using more features, should I be using more overfitting safeguards, like batchnorm + dropout?\n",
    "I tried that, it didn't really help, 74.84% val acc ;-(\n",
    "\n",
    "When I tried a different lr schedule (15 epochs $\\alpha = 2.5e-5$, and then another 15 with $\\alpha=5e-6$), I got to 76.55% acc.\n",
    "\n",
    "### Downsampling ALL layers\n",
    "\n",
    "When we talked with Michael, he said he really didn't have an intuition for whether it would be helpful or not to upsample to the larger dimension to get the dimensions the same. The experiments that I conducted above show \n",
    "\n",
    "1. I wrote some new downsampling layers that use 2d convolutions and downsampling layers to compress the image. The default filters from the 3d-convolutions no longer fit, so I decided first to not compress the image, but use $3 \\times 3 \\times 3$ filters with a stride of 1 to compress info from across the image: 10 epochs 2.5e-5, 5 epochs 2.5e-6: 76.57%\n",
    "\n",
    "2. Next: Try adding 2d BatchNorm in the pre-convolutional layers (it was acutally really stupid of me not to have tried this before) 5 epochs for 2.5e-5, 2.5e-6, 1e-6 $\\rightarrow 77.3%$. *First time I broke 77% with a convolutional model!!!*\n",
    "But I feel like the major lesson here is it's *very* important to be careful with these preconvolutional layers to not wash away too much of the information.\n",
    "\n",
    "There are *a lot* of params in the current network right now: 186k! I saw with the fc when I increased the number of parameters, the performance got *worse*, so try to find some ways to *reduce* the number of parameters. \n",
    "\n",
    "The bottleneck is probably the connection from the 3d conv to the fc layers.\n",
    "\n",
    "3. Increase the stride of the last 3d convolutional layer (need to adjust the filters a bit as well): filter_2= (3,3,2), stride_2=(2,2,2), padding_2=(1,1,1). This decreases the size of the image after this layer, and increases\n",
    "~ 90k parameters, just about the same as my best performing fc net.\n",
    "    - After training *only* for 5 epochs ($\\alpha = 2.5e-5 \\rightarrow$) 77.2% accuracy!!\n",
    "    - Another 5 epochs $\\alpha = 2.5e-6 \\rightarrow 77.6%$ accuracy\n",
    "    \n",
    "4. Try increasing the number of filters for the 2d convolutions from 16 to 32. \n",
    "   After 5 epcohs, $\\alpha = 2.5e-5$, I got 77.4% accuracy. Then 10 epochs, $\\alpha = 1e-5$, 77.6%. 10 epochs $\\alpha = 2e-6$, 77.84%. 3 epochs $\\alpha = 1e-6$ (b/c I needed to go to class), 77.6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameter weights are changed from their initial values, so this looks ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(loader_val, myModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingMetrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "particles = ['gamma','pi','e']\n",
    "\n",
    "for node in range(3):\n",
    "    g_eff, pi_eff, e_eff = sigBkgEff(myModel, loader_val, node)\n",
    "    mTag = '{}-sig_{}'.format(particles[node], myModel.modelName)\n",
    "    \n",
    "    # Plot the ROC curve using node as the signal and the other two as the bkg\n",
    "    if node == 0:\n",
    "        plotROC([g_eff, g_eff], [pi_eff, e_eff], ['$\\pi^+$ rej', '$e^+$ rej'], title='$\\gamma$ signal', tag=mTag)\n",
    "    if node == 1:\n",
    "        plotROC([pi_eff, pi_eff], [g_eff, e_eff], ['$\\gamma$ rej', '$e^+$ rej'], title='$\\pi^+$ signal', tag=mTag)\n",
    "    elif node == 2:\n",
    "        plotROC([e_eff, e_eff], [g_eff, pi_eff], ['$\\gamma$ rej', '$\\pi^+$ rej'], title='$e^+$ signal', tag=mTag)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotConfusion(myModel, loader_val, title=\"Multi Layer Perceptron Baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals:\n",
    "- Print the number of parameters for each model\n",
    "- Just change hyperparameters (by hand) to get more intuition for the problem\n",
    "- Maybe try downsampling to (6,3) instead of upsampling to (12,12)\n",
    "- Could try adding a learning rate scheduler\n",
    "- Try changing the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent CNN\n",
    "\n",
    "- Apply three 2d convolutions to get a fixed size for the image vector!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
